# GemmaGPTLocal
Running Google Gemma 2B instruct model locally using LM Studio, Python, HTML and JavaScript.

### Features
- Runs locally on your computer without the need for an internet connection.
- Utilizes the Google Gemma 2B instruct model for generating responses.
- Gemma is a family of lightweight LLMs built from the same research and technology Google used to create the Gemini models.
- Provides a chat-like interface for seamless interaction

### How to get Started?

1. Clone the GitHub repository

```bash
git clone https://github.com/Solly101/GemmaGPTLocal.git
```
2. Install the required dependencies:

```bash
pip install openai
pip install flask
```
3. Download and install the [LM Studio desktop app](https://lmstudio.ai/). Download the Google gemma 2B instruct model.

4. Expose the  Google gemma 2B instruct model as an OpenAI API by starting the server in LM Studio. 

5. Run the app.py file.

6. Follow the link in the output, then you can chat with the model locally.
   
   


## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
